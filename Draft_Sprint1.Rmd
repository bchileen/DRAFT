---
title: "DRAFT_Cleaning"
output: html_document
date: "2024-12-04"
---

```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(lubridate)
library(caret)
library(ggformula)

dredge_data<-read_csv("DredgeData.csv")
survey_data<-read_csv("SurveyData.csv")
gage_data<-read_csv("UMR_IWW_1999_2021.csv")


gage_data2<- gage_data |>
  group_by(Date)|> 
   summarize(across(everything(),~first(na.omit(.)), .names = "{.col}"))|>
  mutate(DATE_START= as_date(dmy(Date)))|>
    arrange(DATE_START) 
gage_data2<-gage_data2|>
    mutate(across(c(-Date,-DATE_START), 
                ~ ifelse(is.na(.), 
                         coalesce((lag(.) + lead(.)) / 2, lag(.), lead(.)), 
                         .)))|>
  select(-Date)


survey_Data<- survey_data |>
  filter(SURVEYTYPE == "conditional",
         SURVEY_YEAR >= 1999 & SURVEY_YEAR <= 2022,
          RIVER == "Mississippi_River" | RIVER == "Illinois_Waterway",
         SURVEYJOBIDPK == str_replace_all(SURVEYJOBIDPK, "IL", "IW"),
         !POOL %in% c("LP","CS"))|>
  mutate(VOLUMEDREDGED = 0, 
         DREDGINGPURPOSE = "None",
         DREDGE_TYPE = "None",
         LABOR_TYPE = LABOR_SOURCE,
         EXECYEAR = SURVEY_YEAR,
         FEATURENAME = SDSFEATURENAME,
         DATE_START = as.character(DATE_START),
         DATE_START = as_date(DATE_START)
         )|>
  select(SURVEYJOBIDPK,EXECYEAR,DATE_START,DREDGINGPURPOSE,VOLUMEDREDGED, RIVER,POOL, DREDGE_TYPE,UP_RIV_MIL,DN_RIV_MIL)



dredge_data_cleaning <- dredge_data |>
   filter(
          DREDGINGPURPOSE != "harbor",
           EXECYEAR <= 2022)|>
  mutate(
    river_code = str_sub(PREHYDROSURVEY_ID, 1, 2),
    pool_code = str_sub(PREHYDROSURVEY_ID, 4, 5),
    district_code = str_sub(PREHYDROSURVEY_ID, 7, 9),
    river_mile = str_sub(PREHYDROSURVEY_ID, 23, 30), 
    year_iteration = str_sub(PREHYDROSURVEY_ID, 32,34),
    SURVEYJOBIDPK = ifelse(
      str_detect(PREHYDROSURVEY_ID, "MVRUM|MVRIW"), 
      PREHYDROSURVEY_ID, 
      paste0(district_code, river_code, pool_code, "_", river_mile, "_", year_iteration)  
    ),
      SURVEYJOBIDPK = str_replace_all(SURVEYJOBIDPK, "IL", "IW"), 
     DATE_START = as_date(mdy_hm(DATE_START)),
    SDSFEATURENAME = FEATURENAME)|>
  select(SURVEYJOBIDPK,EXECYEAR,DREDGINGPURPOSE, VOLUMEDREDGED, RIVER,POOL, DREDGE_TYPE,DATE_START, UP_RIV_MIL,DN_RIV_MIL)



combined_data <- bind_rows(survey_Data, dredge_data_cleaning)

combined_data <-combined_data |>
     filter(DATE_START >= as.Date("1999-04-01") & 
         DATE_START <= as.Date("2021-09-30"))


combined_data <-combined_data |>
  mutate(lead_7 = (DATE_START - 7),
         lead_14 = (DATE_START - 14))
Nas<- combined_data|>
  filter(if_any(everything(),is.na))

gage_survey <- combined_data |>
  left_join(gage_data2, by = "DATE_START") 


na<-gage_survey |>
  select(everything()) |>
  summarise_all(list(~sum(is.na(.))))
na

NA_by_row<- gage_survey |>
  is.na() |>
  rowSums()

gf_histogram(~NA_by_row)


gage_survey_filt <- gage_survey |>
  filter(complete.cases(gage_survey))



gage_survey_full <- gage_survey_filt|>  
  left_join(gage_data2, by = c("lead_7" = "DATE_START"), suffix = c("", "_7"))|>
  left_join(gage_data2, by = c("lead_14" = "DATE_START"), suffix = c("","_14")) 

Nas<- gage_survey_full|>
  select(-lead_7,-lead_14)|>
  filter(if_any(everything(),is.na))

gage_survey_cleaned <- gage_survey_full |>
  filter(complete.cases(gage_survey_full))

final_na<-gage_survey_cleaned|>
 select(-lead_7,-lead_14)|>
  filter(if_any(everything(),is.na))

### Create factored season variable that could be used in analysis
date_to_season <- function(date) {
  month <- as.numeric(format(date, "%m"))
  day <- as.numeric(format(date, "%d"))
  # Define the seasons based on date ranges
  if ((month == 3 && day >= 20) || (month >= 4 && month <= 6) || (month == 6 && day <= 20)) {
    return("Spring")
  } else if ((month == 6 && day >= 21) || (month >= 7 && month <= 9) || (month == 9 && day <= 21)) {
    return("Summer")
  } else if ((month == 9 && day >= 22) || (month >= 10 && month <= 12) || (month == 12 && day <= 20)) {
    return("Fall")
  } else {
    return("Winter")
  }
}

gage_survey_cleaned <- gage_survey_cleaned |>
  mutate("Season" = sapply(DATE_START,date_to_season))



```


```{r}
gage_survey_PCA <- gage_survey_cleaned |>
select(where(is.numeric), -EXECYEAR)

dredge_numeric<-gage_survey_cleaned|>
   select(where(is.numeric), -EXECYEAR)
correlations <-cor(dredge_numeric, use = "pairwise.complete.obs")
corrplot::corrplot(correlations)

dredge_prcomp <- gage_survey_PCA|>
  select(-VOLUMEDREDGED)

dredge_PCA = prcomp(dredge_prcomp, scale = T)
biplot(dredge_PCA)

variable_contributions<- dredge_PCA$rotation
head(variable_contributions)

set.seed(12)
ctrl = trainControl(method = "cv", number = 5) 

fit_pca = train(VOLUMEDREDGED ~ ., 
            data = gage_survey_PCA,
            method = "glm",
            preProc = c("pca"),
            trControl = ctrl)
fit_pca

summary(fit_pca$finalModel)
varImp(fit_pca)
variable_contributions = fit_pca$preProcess$rotation
head(variable_contributions)

data.frame(variable_contributions) |>
  gf_point(PC2 ~ PC1)
  
data.frame(variable_contributions) |>
  gf_text(PC2 ~ PC1, 
          label = row.names(variable_contributions))

predictor_matrix <- gage_survey_PCA |>
  dplyr::select(-VOLUMEDREDGED) %>% # Remove the response variable
  as.matrix() %>%
  scale() #centers and scales

# The symbol %*% does matrix multiplication
data_in_PC_space = predictor_matrix %*% variable_contributions

data.frame(data_in_PC_space) %>%
  gf_point(PC2 ~ PC1)

```

###xgBoost
```{r}
library(xgboost)
gage_xgb <- gage_survey |>
  select(-Date_obvs,-Date_7,-Date_14,-DATE_START,
         -lead_7,-lead_14,-SURVEYJOBIDPK,-EXECYEAR,-DREDGE_TYPE,
         -DREDGINGPURPOSE)|>
    mutate_if(is.character, factor)
  
ctrl = trainControl(method = "cv", number = 10) # Increase this to 10 to get more consistent RMSEs
fit_dredge = train(VOLUMEDREDGED ~ ., 
                 data = gage_xgb,
                 method = "xgbTree",
                 tuneGrid = expand.grid(nrounds = c (15,25,50,100), max_depth = 1:4, 
                                        eta = 0.3, gamma = 0, 
                                        colsample_bytree = .8, 
                                        min_child_weight = 1, subsample = 1),
                 verbosity = 0, 
                 trControl = ctrl, 
                 na.action = na.exclude)

varImp(fit_dredge)

import_matrix = xgb.importance(model = fit_dredge$finalModel)
import_matrix
xgb.plot.importance(import_matrix)
```
###Cross Validation of Model
```{r}
set.seed(123)
n = nrow(gage_xgb)
n_outer_folds = 10

cv_pred = vector(length = n)

groups = rep(1:n_outer_folds, length = n)
cv_groups = sample(groups, n)


for(ii in 1:n_outer_folds){
  in_test = (cv_groups == ii)
  in_train = (cv_groups != ii)
  
  
  ctrl = trainControl(method = "cv", number = 5)
  fit_dredge_DCV = train(VOLUMEDREDGED ~ ., 
                       data = gage_xgb[in_train, ],
                       method = "xgbTree",
                       tuneGrid = expand.grid(nrounds = c (15,25,50,100), max_depth = 1:6,
                                        eta = 0.3, gamma = 0, 
                                        colsample_bytree = 1, 
                                        min_child_weight = 1, subsample = 1),
                       verbosity = 0, 
                       trControl = ctrl,
                       na.action = na.exclude)

  
  # "Frankenstein" vector of predictions
  cv_pred[in_test] = predict(fit_dredge_DCV, 
                             newdata = gage_xgb[in_test, ])
  
}

fit_dredge_DCV

confusion_matrix <- confusionMatrix(cv_pred[in_test], gage_xgb[in_test, ]$VOLUMEDREDGED)
print(confusion_matrix)

# I'll use RMSE to assess the model, because that's the metric I used to select the model.
# The model with the best RMSE may or may not be the one with the best MAE.
# RMSE
sqrt(mean((cv_pred - gage_xgb$VOLUMEDREDGED)^2)) # This is the most honest 
                                         # assessment of accuracy




dredge_data_cleaning|>
  group_by(POOL)|>
  summarize(count())
```

