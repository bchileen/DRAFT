---
title: "DRAFT_Cleaning"
output: html_document
date: "2024-12-04"
---

```{r setup, include=FALSE}
library(readr)
library(tidyverse)
library(lubridate)
library(caret)
library(ggfortify)
library(ggformula)
library(DiagrammeR)
library(gridExtra)
library(keras)
library(tensorflow)


### Key Functions 
# Function to plot PCA
plot_pca <- function(PCAvalues, top_loadings, PC1_explained, PC2_explained, grouping_var) {
  
  p <- ggplot(PCAvalues, aes(x = "PC1", y = "PC2", colour = grouping_var)) +
    geom_point(size = 2) +  # Scatter plot
    geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
                 arrow = arrow(length = unit(1/2, "picas")), color = "black") +  # Vectors for top loadings
    annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
             label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  # Labels for top loadings
    theme_minimal() +
    labs(title = paste("PCA Plot with Top 10 Loadings for PC1 and PC2 - Grouped by", grouping_var),
         x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
         y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))
  
  return(p)
}

#Function to calculate season from date 
date_to_season <- function(date) {
  month <- as.numeric(format(date, "%m"))
  day <- as.numeric(format(date, "%d"))
  # Define the seasons based on date ranges
  if ((month == 3 && day >= 20) || (month >= 4 && month <= 6) || (month == 6 && day <= 20)) {
    return("Spring")
  } else if ((month == 6 && day >= 21) || (month >= 7 && month <= 9) || (month == 9 && day <= 21)) {
    return("Summer")
  } else if ((month == 9 && day >= 22) || (month >= 10 && month <= 12) || (month == 12 && day <= 20)) {
    return("Fall")
  } else {
    return("Winter")
  }
}

```

### Read in the data
```{r}
dredge_data<-read_csv("DredgeData.csv", show_col_types = FALSE)
survey_data<-read_csv("SurveyData.csv", show_col_types = FALSE)
gage_data<-read_csv("UMR_IWW_1999_2024.csv", show_col_types = FALSE)
```

### Clean the gage data 
The gage data has multiple records across different gages per day, we need to
combine the rows by date so that there's only one record for date across all gages.
Since we're going to be doing a lot of work with the date column, lets format it as 
a date object and arrange by DATE_START and remove the old Date column
```{r}
gage_data<- gage_data |>
  group_by(Date)|>
   summarize(across(everything(),~first(na.omit(.)), .names = "{.col}"))|>
  mutate(DATE_START= as_date(dmy(Date)))|>
    arrange(DATE_START)|>
  select(-Date)

### Some of the gage data has observation gaps where the sensors may not have been
### working, lets interpolate gage values where there's data before and after a gap
### with a maximum window of 2
gage_data<-gage_data|>
    mutate(across(c(-DATE_START),
                ~ ifelse(is.na(.),
                         coalesce((lag(.) + lead(.)) / 2, lag(.), lead(.)),
                         .)))
```

### Cleaning the data
Now lets clean the survey data. I only want conditional survey records between
1999 to 2022, these are going to act as my background data. I want to combine
these records with my dredge data. I'm also going to filter by river, remove
pools that are not of interest, and create some 
columns to support my join. Dates are also formatted as date objects and some
columns are renamed to make it easier to join my surveys to my dredge data.
Finally, select the columns we need for our analysis to join to our dredging 
data. 
```{r}
survey_data<- survey_data |>
  filter(SURVEYTYPE == "conditional",
         SURVEY_YEAR >= 1999 & SURVEY_YEAR <= 2024,
          RIVER == "Mississippi_River" | RIVER == "Illinois_Waterway",
         !POOL %in% c("LP","CS"))|>
  mutate(VOLUMEDREDGED = 0,
         DREDGINGPURPOSE = "None",
         DREDGE_TYPE = "None",
         LABOR_TYPE = LABOR_SOURCE,
         EXECYEAR = SURVEY_YEAR,
         FEATURENAME = SDSFEATURENAME,
         DATE_START = as.character(DATE_START),
         DATE_START = as_date(DATE_START)
         )|>
  select(EXECYEAR,DATE_START,DREDGINGPURPOSE,VOLUMEDREDGED, RIVER,POOL, DREDGE_TYPE,
         UP_RIV_MIL,DN_RIV_MIL)
```

Lets clean the dredge data, we only want records that are channel dredging events
and since the dredge records only go back to 1999, we just need to filter out data beyond 2022.
```{r}
dredge_data <- dredge_data |>
   filter(
          DREDGINGPURPOSE != "harbor",
           EXECYEAR <= 2024)|>
  mutate(
     DATE_START = as_date(mdy_hm(DATE_START)))|>
  select(EXECYEAR,DREDGINGPURPOSE, VOLUMEDREDGED, RIVER,POOL, DREDGE_TYPE,
         DATE_START, UP_RIV_MIL,DN_RIV_MIL)
```

Combine the dredge data and survey data into 1 table. The survey data will act as background/non-dredge data.
```{r}
combined_data <- bind_rows(survey_data, dredge_data)

combined_data <-combined_data |>
     filter(DATE_START >= as.Date("1999-04-01") &
         DATE_START <= as.Date("2024-12-31")) |>
          mutate( DREDGINGPURPOSE = case_when(DREDGINGPURPOSE == "None" ~ "No Dredging",
                                      DREDGINGPURPOSE == "channel" ~ "Dredged"))


combined_data <-combined_data |>
  mutate(lead_7 = (DATE_START - 7),
         lead_14 = (DATE_START - 14))
```

Check for NAs for PCA analysis. 
```{r}
Nas<- combined_data|>
  filter(if_any(everything(),is.na))

gage_survey <- combined_data |>
  left_join(gage_data, by = "DATE_START")


na<-gage_survey |>
  select(everything()) |>
  summarise_all(list(~sum(is.na(.))))
na

NA_by_row<- gage_survey |>
  is.na() |>
  rowSums()

gf_histogram(~NA_by_row)


gage_survey_filt <- gage_survey |>
  filter(complete.cases(gage_survey))

write_csv(gage_survey_filt,"Gage_Survey_Dredge.csv")
```

Join the gage data by the 7 day prior observations and by the 14 day prior observations
```{r}
gage_survey_full <- gage_survey_filt|>  
  left_join(gage_data, by = c("lead_7" = "DATE_START"), suffix = c("", "_7"))|>
  left_join(gage_data, by = c("lead_14" = "DATE_START"), suffix = c("","_14"))
```

Check again for NAs, remove any remaining NA rows. 
```{r}
Nas<- gage_survey_full|>
  select(-lead_7,-lead_14)|>
  filter(if_any(everything(),is.na))

gage_survey_cleaned <- gage_survey_full |>
  filter(complete.cases(gage_survey_full))

final_na<-gage_survey_cleaned|>
 select(-lead_7,-lead_14)|>
  filter(if_any(everything(),is.na))

```

Create factored season variable that could be used in analysis
```{r}
gage_survey_cleaned <- gage_survey_cleaned |>
  mutate("Season" = sapply(DATE_START,date_to_season))

```

Lets make some plots of the data!
```{r}
gages_pivoted <- gage_data|>
    pivot_longer(cols = -DATE_START, 
               names_to = "gage", 
               values_to = "value")



gage_survey_cleaned |>
  ggplot(aes(y=VOLUMEDREDGED, x=DATE_START))+
  geom_line()

gages_pivoted |>
  ggplot(aes(y = value, x = DATE_START, color = gage, group = gage)) +
  geom_line(size = 1) +  # Adjust line thickness
  scale_color_viridis_d() +  # A color scale that's visually appealing
  labs(
    title = "Time Series of Gage Values",
    x = "Date",
    y = "Value",
    color = "Gage"
  ) +  # Adding titles and labels
  theme_minimal() +  # Use a minimal theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for better readability
    axis.title = element_text(size = 12),  # Increase title font size
    legend.position = "right",  # Position the legend on the right
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10)
  )
```

```{r}
gage_survey |>
  filter(RIVER == "Mississippi_River")|>
  ggplot(aes(y=VOLUMEDREDGED, x=DATE_START,color = POOL))+
  geom_point()+
  labs(title = "Miss Dredging")

gage_survey |>
  filter(RIVER == "Illinois_Waterway")|>
  ggplot(aes(y=VOLUMEDREDGED, x=DATE_START, color = POOL))+
  geom_point()+
  labs(title = "IWW Dredging")

```



Now on to analysis!


To first explore the variables, we will use PCA.
### PCA
PCA using prcomp
```{r}
gage_survey_PCA <- gage_survey_cleaned |>
select(where(is.numeric), -EXECYEAR)

dredge_prcomp <- gage_survey_PCA|>
  select(-VOLUMEDREDGED)

dredge_PCA = prcomp(dredge_prcomp, scale = T)

plot(dredge_PCA,  type="l")

a<-autoplot(dredge_PCA,data = gage_survey_cleaned, 
         color = 'Season',loadings = FALSE)+
  ggtitle("Season")


b<-autoplot(dredge_PCA,data = gage_survey_cleaned, 
         color = 'POOL',loadings= FALSE)+
  ggtitle("River Pool")+
    guides(color = guide_legend(ncol = 2)) 


c<-autoplot(dredge_PCA,data = gage_survey_cleaned, 
         color = 'RIVER',loadings = FALSE)+
  ggtitle("River")

d<-autoplot(dredge_PCA,data = gage_survey_cleaned, 
         color = 'DREDGINGPURPOSE',loadings = FALSE)+
        ggtitle("Dredging Purpose")+
  labs(color = "Dredging?")



ggsave("Initial_Full_PCA.png",grid.arrange(a,b,c,d,ncol = 2),width = 10, height = 8)
variable_contributions<- dredge_PCA$rotation
head(variable_contributions)
```

PCA using Caret
```{r}
set.seed(12)
ctrl = trainControl(method = "cv", number = 5)

fit_pca = caret::train(VOLUMEDREDGED ~ .,
            data = gage_survey_PCA,
            method = "glm",
            preProc = c("pca"),
            trControl = ctrl)
fit_pca

summary(fit_pca$finalModel)
varImp(fit_pca)
variable_contributions = fit_pca$preProcess$rotation
head(variable_contributions)


data.frame(variable_contributions) |>
  gf_text(PC2 ~ PC1,
          label = row.names(variable_contributions))

data.frame(variable_contributions) |>
  filter(abs(PC1) > 0.11)

predictor_matrix <- gage_survey_PCA |>
  dplyr::select(-VOLUMEDREDGED) |> # Remove the response variable
  as.matrix() |>
  scale() #centers and scales



# The symbol %*% does matrix multiplication
data_in_PC_space = predictor_matrix %*% variable_contributions

data_in_PC_space = data.frame(data_in_PC_space,
                              River = gage_survey_cleaned$RIVER,
                              Pool = gage_survey_cleaned$POOL,
                              DREDGE_TYPE = gage_survey_cleaned$DREDGINGPURPOSE,
                              Season = gage_survey_cleaned$Season,
                              Dredge_Volume = gage_survey_cleaned$VOLUMEDREDGED)

data.frame(data_in_PC_space)|>
  gf_point(PC2 ~ PC1, col = ~ Season)

data.frame(data_in_PC_space)|>
  gf_point(PC2 ~ PC1, col = ~DREDGE_TYPE)


### Scree Plot
data_in_PC_space = predictor_matrix %*% variable_contributions
var_explained = apply(data_in_PC_space,2,var)
pve = var_explained/sum(var_explained)
pve

cumsum(pve)

var_df <- data.frame(pve, PC = 1:length(pve),
                     cum_var = cumsum(pve))

var_df|>
  gf_point(cum_var ~ PC) |>
  gf_line(cum_var ~ PC)
```

###xgBoost
```{r}
library(xgboost)
gage_xgb <- gage_survey_cleaned |>
  select(-lead_7, -lead_14,-DATE_START,
         -EXECYEAR,-DREDGE_TYPE,
         -DREDGINGPURPOSE)|>
    mutate_if(is.character, factor)
 
ctrl = trainControl(method = "cv", number = 5) 
fit_dredge = caret::train(VOLUMEDREDGED ~ .,
                 data = gage_xgb,
                 method = "xgbTree",
               tuneGrid = expand.grid(nrounds = c (15,25,50,100), 
                                              max_depth = 1:10,
                                        eta =c(0.3), gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = 1, subsample = 1),
                 verbosity = 0,
                 trControl = ctrl,
                 na.action = na.exclude)

varImp(fit_dredge)
xgb.plot.tree(model = fit_dredge$finalModel, trees = 0)

fit_dredge

import_matrix = xgb.importance(model = fit_dredge$finalModel)
import_matrix
xgb.plot.importance(import_matrix)

```
###Cross Validation of Model
```{r}
set.seed(123)
n = nrow(gage_xgb)
n_outer_folds = 5

cv_pred = vector(length = n)

groups = rep(1:n_outer_folds, length = n)
cv_groups = sample(groups, n)


for(ii in 1:n_outer_folds){
  in_test = (cv_groups == ii)
  in_train = (cv_groups != ii)
 
 
  ctrl = trainControl(method = "cv", number = 5)
  fit_dredge_DCV = caret::train(VOLUMEDREDGED ~ .,
                       data = gage_xgb[in_train, ],
                       method = "xgbTree",
                       tuneGrid = expand.grid(nrounds = c (15,25,50,100), 
                                              max_depth = 1:10,
                                        eta =c(0.3), gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = 1, subsample = 1),
                       verbosity = 0,
                       trControl = ctrl,
                       na.action = na.exclude)
 
  # "Frankenstein" vector of predictions
  cv_pred[in_test] = predict(fit_dredge_DCV,
                             newdata = gage_xgb[in_test, ])
 
}


##Og
train<-gage_xgb[in_train, ]
best_tune<-fit_dredge_DCV$results
best_tune
confusion_matrix <- confusionMatrix(cv_pred[in_test], gage_xgb[in_test, ])
print(confusion_matrix)

# I'll use RMSE to assess the model, because that's the metric I used to select the model.
# The model with the best RMSE may or may not be the one with the best MAE.
# RMSE
sqrt(mean((cv_pred - gage_xgb$VOLUMEDREDGED)^2)) # This is the most honest
                                         # assessment of accuracy
```

## Well that didnt work! Lets try modeling between the two river systems 

### Cleaning the data 
```{r}
combined_data <- bind_rows(survey_data, dredge_data)

combined_data <-combined_data |>
     filter(DATE_START >= as.Date("1999-04-01") &
         DATE_START <= as.Date("2024-12-31"))

combined_data <-combined_data |>
  mutate(lead_7 = (DATE_START - 7),
         lead_14 = (DATE_START - 14))
Nas<- combined_data|>
  filter(if_any(everything(),is.na))

gage_survey <- combined_data |>
  left_join(gage_data, by = "DATE_START")

gage_IWW <- gage_data |>
  select(DATE_START,BEAI2,HAVI2,
         HNYI2,IL03, IL03_Tail, IL04, IL04_Tail, IL05, IL05_Tail,IL06, IL06_Tail,
         IL07, IL07_Tail, IL08, IL08_Tail,KNGI2, MORI2, PIAI2,"05552500","05527500",
         "05585000","05583000","05570000","05555300")

gage_Miss <- gage_data |>
  select(DATE_START,BRLI4,CMMI4, DBQI4,
         FAII4, KHBI2,MI11,MI11_Tail,MI12,MI12_Tail,MI13,MI13_Tail,MI14,MI14_Tail,
         MI15,MI15_Tail,MI16,MI16_Tail,MI17,MI17_Tail,MI18,MI18_Tail,MI19,MI19_Tail,
         MI20,MI20_Tail,MI21,MI21_Tail,MI22,MI22_Tail, MUSI4,UINI2,"05446500")



Illinois_River <- gage_survey |>
  select(EXECYEAR,DATE_START,DREDGINGPURPOSE,VOLUMEDREDGED,RIVER,
         POOL, DREDGE_TYPE,UP_RIV_MIL, DN_RIV_MIL,lead_7,lead_14,BEAI2,HAVI2,
         HNYI2,IL03, IL03_Tail, IL04, IL04_Tail, IL05, IL05_Tail,IL06, IL06_Tail,
         IL07, IL07_Tail, IL08, IL08_Tail,KNGI2, MORI2, PIAI2,"05552500","05527500",
         "05585000","05583000","05570000","05555300") |>
  filter(RIVER == "Illinois_Waterway")

   

Miss_River <- gage_survey |>
  select(EXECYEAR,DATE_START,DREDGINGPURPOSE,VOLUMEDREDGED,RIVER,
         POOL, DREDGE_TYPE,UP_RIV_MIL, DN_RIV_MIL,lead_7,lead_14,BRLI4,CMMI4, DBQI4,
         FAII4, KHBI2,MI11,MI11_Tail,MI12,MI12_Tail,MI13,MI13_Tail,MI14,MI14_Tail,
         MI15,MI15_Tail,MI16,MI16_Tail,MI17,MI17_Tail,MI18,MI18_Tail,MI19,MI19_Tail,
         MI20,MI20_Tail,MI21,MI21_Tail,MI22,MI22_Tail, MUSI4,UINI2,"05446500") |>
  filter(RIVER == "Mississippi_River")

Miss_full <- Miss_River|>  
  left_join(gage_Miss, by = c("lead_7" = "DATE_START"), suffix = c("", "_7"))|>
  left_join(gage_Miss, by = c("lead_14" = "DATE_START"), suffix = c("","_14"))

IWW_full <- Illinois_River|>  
  left_join(gage_IWW, by = c("lead_7" = "DATE_START"), suffix = c("", "_7"))|>
  left_join(gage_IWW, by = c("lead_14" = "DATE_START"), suffix = c("","_14"))


Miss_Cleaned <- Miss_full |>
  filter(complete.cases(Miss_full))

IWW_Cleaned <- IWW_full |>
  filter(complete.cases(IWW_full))

IWW_Cleaned <- IWW_Cleaned |>
  mutate("Season" = sapply(DATE_START,date_to_season))


Miss_Cleaned <- Miss_Cleaned |>
  mutate("Season" = sapply(DATE_START,date_to_season))

```

## IWW PCA
```{r}
IWW_PCA <- IWW_Cleaned |>
select(where(is.numeric), -EXECYEAR)

IWW_prcomp <- IWW_PCA|>
  select(-VOLUMEDREDGED)

IWW_PCA = prcomp(IWW_prcomp, scale = T)

biplot(IWW_PCA)

plot(IWW_PCA,  type="l")

autoplot(IWW_PCA,data = IWW_Cleaned, 
         color = 'Season',loadings = TRUE,
         loadings.colour = 'black',
         loadings.label = TRUE, loadings.label.size = 4, 
         loadings.label.colour = "blue")

PCAvalues <- data.frame(Pool = IWW_Cleaned$POOL,
                        Dredge_Type = IWW_Cleaned$DREDGE_TYPE,
                        Season = IWW_Cleaned$Season,
                        Dredge_Occurance = IWW_Cleaned$DREDGINGPURPOSE,
                        IWW_PCA$x)

explained_variance <- (IWW_PCA$sdev^2) / sum(IWW_PCA$sdev^2) * 100

# Extract PC1 and PC2 explained variance
PC1_explained <- explained_variance[1]
PC2_explained <- explained_variance[2]

# Extract loadings of the variables
PCAloadings <- data.frame(Variables = rownames(IWW_PCA$rotation), IWW_PCA$rotation)


PCAloadings_PC1 <- PCAloadings|>
  arrange(desc(abs(PC1))) |>
  slice_head(n=10)
  
PCAloadings_PC2 <- PCAloadings|>
  arrange(desc(abs(PC2))) |>
  slice_head(n=10)
  
top_loadings <- bind_rows(PCAloadings_PC1, PCAloadings_PC2)

# Plot PCA with top 10 loadings
ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Season)) +
  geom_point(size = 2) +
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +  
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))

ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Pool)) +
  geom_point(size = 2) +
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +  
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))


ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Dredge_Type)) +
  geom_point(size = 2) + 
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") + 
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))


ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Dredge_Occurance)) +
  geom_point(size = 2) + 
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) + 
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))
```

## Miss PCA 
```{r}
Miss_PCA <- Miss_Cleaned |>
select(where(is.numeric), -EXECYEAR)

Miss_prcomp <- Miss_PCA|>
  select(-VOLUMEDREDGED)

Miss_PCA = prcomp(Miss_prcomp, scale = T)

biplot(Miss_PCA)
biplot(Miss_PCA, showLoadings = TRUE,)

plot(Miss_PCA,  type="l")

PCAvalues <- data.frame(Pool = Miss_Cleaned$POOL,
                        Dredge_Type = Miss_Cleaned$DREDGE_TYPE,
                        Season = Miss_Cleaned$Season,
                        Dredge_Occurance = Miss_Cleaned$DREDGINGPURPOSE,
                        Miss_PCA$x)

explained_variance <- (Miss_PCA$sdev^2) / sum(Miss_PCA$sdev^2) * 100

# Extract PC1 and PC2 explained variance
PC1_explained <- explained_variance[1]
PC2_explained <- explained_variance[2]

# Extract loadings of the variables
PCAloadings <- data.frame(Variables = rownames(Miss_PCA$rotation), Miss_PCA$rotation)


PCAloadings_PC1 <- PCAloadings|>
  arrange(desc(abs(PC1))) |>
  slice_head(n=10)
  
PCAloadings_PC2 <- PCAloadings|>
  arrange(desc(abs(PC2))) |>
  slice_head(n=10)
  
top_loadings <- bind_rows(PCAloadings_PC1, PCAloadings_PC2)

# Plot PCA with top 10 loadings
ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Season)) +
  geom_point(size = 2) +  # Scatter plot
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +  # Vectors for top loadings
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  # Labels for top loadings
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))

ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Pool)) +
  geom_point(size = 2) +  # Scatter plot
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +  # Vectors for top loadings
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  # Labels for top loadings
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))

ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Dredge_Type)) +
  geom_point(size = 2) +  # Scatter plot
  geom_segment(data = top_loadings, aes(x = 0, y = 0, xend = PC1 * 75, yend = PC2 * 75),
               arrow = arrow(length = unit(1/2, "picas")), color = "black") +  # Vectors for top loadings
  annotate("text", x = top_loadings$PC1 * 80, y = top_loadings$PC2 * 80,
           label = top_loadings$Variables, hjust = 0.2, vjust = 0.4) +  # Labels for top loadings
  theme_minimal() +
  labs(title = "PCA Plot with Top 10 Loadings for PC1 and PC2",
      x = paste("PC1 (", round(PC1_explained, 1), "%)", sep = ""),
       y = paste("PC2 (", round(PC2_explained, 1), "%)", sep = ""))

```

## xgboost split by river
#Miss
```{r}
Miss_xgb <-Miss_Cleaned |>
  select(-lead_7, -lead_14,-DATE_START,
         -EXECYEAR,-DREDGE_TYPE,
         -DREDGINGPURPOSE, -RIVER)|>
    mutate_if(is.character, factor)
 
ctrl = trainControl(method = "cv", number = 10) 
Miss_xgb_fit = caret::train(VOLUMEDREDGED ~ .,
                 data = Miss_xgb,
                 method = "xgbTree",
               tuneGrid = expand.grid(nrounds = c(15,25,50,100), 
                                              max_depth = 1:10,
                                        eta =c(0.3), gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = 1, subsample = 1),
                 verbosity = 0,
                 trControl = ctrl,
                 na.action = na.exclude)

varImp(Miss_xgb_fit)
xgb.plot.tree(model = Miss_xgb_fit$finalModel, trees = 0)

Miss_xgb_fit

import_matrix = xgb.importance(model = Miss_xgb_fit$finalModel)
import_matrix
xgb.plot.importance(import_matrix)
```

#IWW
```{r}
IWW_xgb <-IWW_Cleaned |>
  select(-lead_7, -lead_14,-DATE_START,
         -EXECYEAR,-DREDGE_TYPE,
         -DREDGINGPURPOSE, -RIVER)|>
    mutate_if(is.character, factor)
 
ctrl = trainControl(method = "cv", number = 10) 
IWW_xgb_fit = caret::train(VOLUMEDREDGED ~ .,
                 data = IWW_xgb,
                 method = "xgbTree",
               tuneGrid = expand.grid(nrounds = c(15,25,50,100), 
                                              max_depth = 1:10,
                                        eta =c(0.3), gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = 1, subsample = 1),
                 verbosity = 0,
                 trControl = ctrl,
                 na.action = na.exclude)

varImp(IWW_xgb_fit)
xgb.plot.tree(model = IWW_xgb_fit$finalModel, trees = 0)

IWW_xgb_fit

import_matrix = xgb.importance(model = IWW_xgb_fit$finalModel)
import_matrix
xgb.plot.importance(import_matrix)
```
###LSTM
```{r}
library(tensorflow)
library(keras)
model_7 <- keras_model_sequential() |>
  layer_lstm(units = 50, return_sequences = TRUE, input_shape = c(7, ncol(gage_xgb) - 1))  |>
  layer_dropout(rate = 0.2)|>
  layer_lstm(units = 50, return_sequences = TRUE) |>
  layer_dropout(rate = 0.2) |>
  layer_lstm(units = 50) |>
  layer_dropout(rate = 0.2) |>
  layer_dense(units = 1)


model_7 %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam'
)


# Function to create sequences based on window size
create_sequences <- function(data, target_column, sequence_length) {
  X <- list()
  y <- list()
  
  for (i in (sequence_length + 1):nrow(data)) {
    X[[i - sequence_length]] <- as.matrix(data[(i - sequence_length):(i - 1), -target_column])  # Features excluding target
    y[[i - sequence_length]] <- data[i, target_column]  # Target column
  }
  
  X <- array(unlist(X), dim = c(length(X), sequence_length, ncol(data) - 1))  # Reshape to 3D array
  y <- unlist(y)
  
  return(list(X = X, y = y))
}

# Example: Create sequences for 7-day and 14-day windows
window_size_7 <- 7
window_size_14 <- 14

# Load necessary libraries
library(recipes)

# Assume gage_xgb is your original dataset
gage_xgb_scaled_recipe <- recipe(~ ., data = gage_xgb) %>%
  # Define step to scale all numeric predictors
  step_scale(all_numeric_predictors()) %>%
  # You can also center them if needed
  step_center(all_numeric_predictors())

# Prep the recipe
gage_xgb_prepped <- prep(gage_xgb_scaled_recipe, training = gage_xgb)

# Apply the scaling to the data
gage_xgb_scaled <- bake(gage_xgb_prepped, new_data = gage_xgb)

# View scaled data
head(gage_xgb_scaled)




# Assume `gage_xgb_scaled` is your scaled dataset and the target column is 'VOLUMEDREDGED'
data_windows_7 <- create_sequences(gage_xgb_scaled, target_column = which(names(gage_xgb) == "VOLUMEDREDGED"), sequence_length = window_size_7)
data_windows_14 <- create_sequences(gage_xgb_scaled, target_column = which(names(gage_xgb) == "VOLUMEDREDGED"), sequence_length = window_size_14)

# X_train and y_train for 7-day window
X_train_7 <- data_windows_7$X
y_train_7 <- data_windows_7$y

# X_train and y_train for 14-day window
X_train_14 <- data_windows_14$X
y_train_14 <- data_windows_14$y

# Define the train-test split
train_size_7 <- floor(0.8 * length(y_train_7))  # 80% for training
train_size_14 <- floor(0.8 * length(y_train_14))  # 80% for training

# Training and Testing Data for 7-day window
X_train_7_final <- X_train_7[1:train_size_7,,]
y_train_7_final <- y_train_7[1:train_size_7]
X_test_7_final <- X_train_7[(train_size_7+1):length(y_train_7),,]
y_test_7_final <- y_train_7[(train_size_7+1):length(y_train_7)]

# Training and Testing Data for 14-day window
X_train_14_final <- X_train_14[1:train_size_14,,]
y_train_14_final <- y_train_14[1:train_size_14]
X_test_14_final <- X_train_14[(train_size_14+1):length(y_train_14),,]
y_test_14_final <- y_train_14[(train_size_14+1):length(y_train_14)]


# Train the model for the 7-day window
history_7 <- model_7 %>% fit(
  X_train_7_final, 
  y_train_7_final,
  epochs = 20, 
  batch_size = 32, 
  validation_data = list(X_test_7_final, y_test_7_final)
)

# Evaluate model on 7-day window
score_7 <- model_7 %>% evaluate(X_test_7_final, y_test_7_final)
cat('Test loss for 7-day window:', score_7, "\n")


predictions_7 <- model_7 %>% predict(X_test_7_final)
```

