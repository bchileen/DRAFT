---
title: "Lesson 12 examples"
author: "Abra Brisbin"
date: "11/15/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(MASS) # For simulating correlated data; load this before dplyr to avoid masking `select`

library(dplyr)
library(ggformula)
library(readr)

library(corrplot)
library(RColorBrewer)


```

## PCA

```{r}

data("segmentationData")

```
```{r}
head(segmentationData)
```

```{r}

segmentationData2 <- segmentationData %>%
  dplyr::select(-c(Cell, Case))

head(segmentationData2)

```

```{r}

seg_numeric = select_if(segmentationData2, is.numeric)
correlations <-cor(seg_numeric, use = "pairwise.complete.obs")

# Here, I'm specifying that I want the version of the `corrplot` function that's in the `corrplot` package, to avoid confusion with a function of the same name in the `pls` package.
corrplot::corrplot(correlations, type="upper", order="hclust",
         col=rev(brewer.pal(n=8, name="RdYlBu")))
  
```

## No PCA
```{r}

set.seed(12)
ctrl = trainControl(method = "cv", number = 5)
fit = train(Class ~ ., 
            data = segmentationData2,
            method = "glm",
            family = "binomial",
            trControl = ctrl)
fit

```

```{r}
summary(fit$finalModel)
```

PCA
```{r}

set.seed(12)
ctrl = trainControl(method = "cv", number = 5) 

fit_pca = train(Class ~ ., 
            data = segmentationData2,
            method = "glm",
            family = "binomial",
            preProc = c("pca"),
            trControl = ctrl)
fit_pca

```



```{r}
summary(fit_pca$finalModel)
```
```{r}
varImp(fit_pca)
```

caret centers and scales the data
```{r}
fit_pca$preProcess
```

View the loadings.  These match the $rotation object from prcomp( ).
```{r}

variable_contributions = fit_pca$preProcess$rotation
head(variable_contributions)

```

```{r}

data.frame(variable_contributions) %>%
  gf_point(PC2 ~ PC1)
  
data.frame(variable_contributions) %>%
  gf_text(PC2 ~ PC1, 
          label = row.names(variable_contributions))

```

```{r}

data.frame(variable_contributions) %>%
  filter(abs(PC1) > .15)

```

Get coordinates of data in PC-space
```{r}

predictor_matrix <- segmentationData2 %>%  
  dplyr::select(-Class) %>% # Remove the response variable
  as.matrix() %>%
  scale() #centers and scales

# The symbol %*% does matrix multiplication
data_in_PC_space = predictor_matrix %*% variable_contributions

# Append the response variable
data_in_PC_space = data.frame(data_in_PC_space, 
                              Class = segmentationData$Class)

```

```{r}

data.frame(data_in_PC_space) %>%
  gf_point(PC2 ~ PC1, col =~ Class)

```

Predicting the first row of the data
```{r}

predict(fit_pca, newdata = segmentationData2[1, ], type = "prob")

```

## Changing the number of PCs
```{r}
set.seed(12)

ctrl = trainControl(method = "cv", number = 5,
                    preProcOptions = list(thresh = 1)) 

#pcaComp = 5 or thresh = .8

fit_pca_all = train( Class ~ ., 
            data = segmentationData2, 
            method = "glm",
            family = "binomial",
            preProc = c("pca"),
            trControl = ctrl)
fit_pca_all
```

Percentage of variation explained (only if you used thresh = 1 or pcaComp = the maximum)
```{r}

variable_contributions = fit_pca_all$preProcess$rotation
data_in_PC_space = predictor_matrix %*% variable_contributions

var_explained = apply(data_in_PC_space, 2, var)
pve = var_explained/sum(var_explained)
pve

cumsum(pve)

```
Scree plot
```{r}

var_df = data.frame(pve, 
                    PC = 1:length(pve),
                    cum_var = cumsum(pve))

var_df %>%
  gf_point(pve ~ PC) %>%
  gf_line(pve ~ PC)

var_df %>%
  gf_point(cum_var ~ PC) %>%
  gf_line(cum_var ~ PC)

```
### prcomp()

```{r}
seg_predictors = segmentationData2[ , -1] # Remove the response variable

pc_info = prcomp(seg_predictors, scale = TRUE)

biplot(pc_info)
```
A biplot combines the graph of loadings and the graph of data points in PC-space into a single graph.  In a case like this, with many variables, it can be hard to read.  I prefer to make these graphs separately, as shown below.

Graph of variable loadings:
```{r}
PC_loadings = data.frame(pc_info$rotation)

PC_loadings %>%
  gf_point(PC2 ~ PC1)

PC_loadings %>%
  gf_text(PC2 ~ PC1, label =~ rownames(.))
```
Graph of data points in PC-space:
```{r}
data_in_PC_space_from_prcomp = data.frame(pc_info$x, Class = segmentationData$Class)

data_in_PC_space_from_prcomp %>%
  gf_point(PC2 ~ PC1, color =~ Class)

```

### Using a for() loop to do a single layer of 5-fold CV, so we can use prcomp() to do PCA without scaling:
```{r}
num_cells = nrow(segmentationData2)

set.seed(12)
rep_1_5 = rep(1:5, length = num_cells)
cv_folds = sample(rep_1_5, num_cells)

predictions_no_scale = vector(length = num_cells)

for(fold in 1:5){
  train = which(cv_folds != fold)
  test = which(cv_folds == fold)
   
  # Do PCA on the training set
  pc_info = prcomp(seg_predictors[train, ], center=TRUE, scale=FALSE)
  
  # Select columns 1-22 for the first 22 PCs, to match fit_pca.
  # Append the response variable.
  prepared_data = data.frame(pc_info$x[ , 1:22], Class = segmentationData2$Class[train])  
  
  # Fit the model to the training set.
  # Could also do this with caret, using no pre-processing and method = "none".
  fit_no_scale = glm(Class ~ ., data = prepared_data[train, ], family = "binomial")
  
  # Get coordinates of test set in PC-space.
  # Convert to a data frame to enable the next step.
  test_set_pc = data.frame(predict(pc_info, seg_predictors[test, ]))

  # Make predictions using the logistic regression model.
  predictions_no_scale[test] = predict(fit_no_scale, newdata = test_set_pc, type = "response")

} # end 5-fold CV

# Compute accuracy
predictions_class = ifelse(predictions_no_scale > 0.5, "WS", "PS")  # Using a 50% threshold
sum(predictions_class == segmentationData2$Class)/length(segmentationData2$Class)

```
In this case, the accuracy without scaling is similar to the accuracy with scaling.  This is a bit surprising, because there is a wide difference in standard deviations of variables in the original data.

```{r}
std_devs = apply(seg_predictors, 2, sd)
summary(std_devs)
```

### Optional:  PLS
Partial Least Squares is a supervised learning technique that works similarly to using PCA as a preprocessing step before linear regression, but it finds linear combinations of the predictors that best represent the covariance between the predictors and the response (rather than just the variance of the predictors).  This means it can often get good predictions using fewer components than needed with PCA.

If you're interested, check out this video:  https://youtu.be/n_Ll9t6iPvE
```{r}
my_sigma = matrix(c(2,1,1,1), nc = 2)
set.seed(12)
example_1 = data.frame(mvrnorm(200, mu = c(0,0), Sigma = my_sigma))
example_1 <- example_1 %>%
  mutate(y = X2 - X1)

example_1 %>%
  gf_point(X2~X1, col =~ y) 
```
```{r}

data(BloodBrain)
head(logBBB)
head(bbbDescr[,1:6])

dim(bbbDescr)
```

```{r}

bbb_numeric = select_if(bbbDescr, is.numeric)
correlations <-cor(bbb_numeric, use = "pairwise.complete.obs")
corrplot::corrplot(correlations, type="upper", order="hclust",
         col=rev(brewer.pal(n=8, name="RdYlBu")))
  
```

```{r}

set.seed(12)

ctrl = trainControl(method = "cv", number = 5)
fit_pcr = train(y = logBBB,
                x = bbbDescr, 
                method = "pcr",
                tuneGrid = expand.grid(ncomp = 6:26), #seq(1, 134, by = 5)
                trControl = ctrl)
fit_pcr

```


```{r}

set.seed(12)

ctrl = trainControl(method = "cv", number = 5)
fit_pls = train(y = logBBB,
                x = bbbDescr, 
                method = "pls",
                tuneGrid = expand.grid(ncomp = 6:26),  #seq(1, 134, by = 5)
                trControl = ctrl)
fit_pls

```

```{r}

varImp(fit_pls)

```
```{r}

coef(fit_pls$finalModel)

````
